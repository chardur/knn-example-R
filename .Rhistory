kmax <- 15
# build the model
model <- train(as.factor(Type)~.,
df_train,
method = "knn", # choose knn model
trControl=trainControl(
method="repeatedcv", # k-fold cross validation
number=10, # number of folds (k in cross validation)
repeats=5), # number of times to repeat k-fold cross validation
preProcess = c("center", "scale"), # standardize the data
tuneLength = kmax) # max number of neighbors (k in nearest neighbor)
plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
main = "K vs Accuracy, 10 fold CV, CARET")
# # tell caret to use cross validation with 10 folds
# trainctl <- trainControl(method = 'cv', number = 10)
#
# # build the model, k values 1-25, careful to scale the data using preproc
# model <- train(as.factor(Type)~.,
#                method = 'knn',
#                tuneGrid = expand.grid(k = 1:25),
#                trControl = trainctl,
#                metric = 'Accuracy',
#                preProc = c("center", "scale"),
#                data = df_train)
#
# plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
#      main = "K vs Accuracy, 10 fold CV, CARET")
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
#library(DataExplorer)
#DataExplorer::create_report(df, y = 'Type')
# shuffle data to minimize random effects
shuffled_df <-  df[sample(nrow(df)),]
# create training 80% and test 20%,
train <- createDataPartition(shuffled_df$Type, p = 0.8, list = FALSE, times = 1)
df_train <- shuffled_df[train,]
df_test <- shuffled_df[-train,]
# k (number of neighbors) to test, try odd numbers, to avoid ties
kmax <- 25
# build the model
model <- train(as.factor(Type)~.,
df_train,
method = "knn", # choose knn model
trControl=trainControl(
method="repeatedcv", # k-fold cross validation
number=10, # number of folds (k in cross validation)
repeats=5), # number of times to repeat k-fold cross validation
preProcess = c("center", "scale"), # standardize the data
tuneLength = kmax) # max number of neighbors (k in nearest neighbor)
plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
main = "K vs Accuracy, 10 fold CV, CARET")
# # tell caret to use cross validation with 10 folds
# trainctl <- trainControl(method = 'cv', number = 10)
#
# # build the model, k values 1-25, careful to scale the data using preproc
# model <- train(as.factor(Type)~.,
#                method = 'knn',
#                tuneGrid = expand.grid(k = 1:25),
#                trControl = trainctl,
#                metric = 'Accuracy',
#                preProc = c("center", "scale"),
#                data = df_train)
#
# plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
#      main = "K vs Accuracy, 10 fold CV, CARET")
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
#library(DataExplorer)
#DataExplorer::create_report(df, y = 'Type')
# shuffle data to minimize random effects
shuffled_df <-  df[sample(nrow(df)),]
# create training 80% and test 20%,
train <- createDataPartition(shuffled_df$Type, p = 0.8, list = FALSE, times = 1)
df_train <- shuffled_df[train,]
df_test <- shuffled_df[-train,]
# k (number of neighbors) to test, try odd numbers, to avoid ties
kmax <- 20
# build the model
model <- train(as.factor(Type)~.,
df_train,
method = "knn", # choose knn model
trControl=trainControl(
method="repeatedcv", # k-fold cross validation
number=10, # number of folds (k in cross validation)
repeats=5), # number of times to repeat k-fold cross validation
preProcess = c("center", "scale"), # standardize the data
tuneLength = kmax) # max number of neighbors (k in nearest neighbor)
plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
main = "K vs Accuracy, 10 fold CV, CARET")
# # tell caret to use cross validation with 10 folds
# trainctl <- trainControl(method = 'cv', number = 10)
#
# # build the model, k values 1-25, careful to scale the data using preproc
# model <- train(as.factor(Type)~.,
#                method = 'knn',
#                tuneGrid = expand.grid(k = 1:25),
#                trControl = trainctl,
#                metric = 'Accuracy',
#                preProc = c("center", "scale"),
#                data = df_train)
#
# plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
#      main = "K vs Accuracy, 10 fold CV, CARET")
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
#library(DataExplorer)
#DataExplorer::create_report(df, y = 'Type')
# shuffle data to minimize random effects
shuffled_df <-  df[sample(nrow(df)),]
# create training 80% and test 20%,
train <- createDataPartition(shuffled_df$Type, p = 0.8, list = FALSE, times = 1)
df_train <- shuffled_df[train,]
df_test <- shuffled_df[-train,]
# k (number of neighbors) to test, try odd numbers, to avoid ties
kmax <- 9
# build the model
model <- train(as.factor(Type)~.,
df_train,
method = "knn", # choose knn model
trControl=trainControl(
method="repeatedcv", # k-fold cross validation
number=10, # number of folds (k in cross validation)
repeats=5), # number of times to repeat k-fold cross validation
preProcess = c("center", "scale"), # standardize the data
tuneLength = kmax) # max number of neighbors (k in nearest neighbor)
plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
main = "K vs Accuracy, 10 fold CV, CARET")
# # tell caret to use cross validation with 10 folds
# trainctl <- trainControl(method = 'cv', number = 10)
#
# # build the model, k values 1-25, careful to scale the data using preproc
# model <- train(as.factor(Type)~.,
#                method = 'knn',
#                tuneGrid = expand.grid(k = 1:25),
#                trControl = trainctl,
#                metric = 'Accuracy',
#                preProc = c("center", "scale"),
#                data = df_train)
#
# plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
#      main = "K vs Accuracy, 10 fold CV, CARET")
View(model)
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
#library(DataExplorer)
#DataExplorer::create_report(df, y = 'Type')
# shuffle data to minimize random effects
shuffled_df <-  df[sample(nrow(df)),]
# create training 80% and test 20%,
train <- createDataPartition(shuffled_df$Type, p = 0.8, list = FALSE, times = 1)
df_train <- shuffled_df[train,]
df_test <- shuffled_df[-train,]
# k (number of neighbors) to test, try odd numbers, to avoid ties
kmax <- 9
# build the model
model <- train(as.factor(Type)~.,
df_train,
method = "knn", # choose knn model
trControl=trainControl(
method="repeatedcv", # k-fold cross validation
number=10, # number of folds (k in cross validation)
repeats=5), # number of times to repeat k-fold cross validation
preProcess = c("center", "scale"), # standardize the data
tuneLength = kmax) # max number of neighbors (k in nearest neighbor)
plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
main = "K vs Accuracy, 10 fold CV, CARET")
# # tell caret to use cross validation with 10 folds
# trainctl <- trainControl(method = 'cv', number = 10)
#
# # build the model, k values 1-25, careful to scale the data using preproc
# model <- train(as.factor(Type)~.,
#                method = 'knn',
#                tuneGrid = expand.grid(k = 1:25),
#                trControl = trainctl,
#                metric = 'Accuracy',
#                preProc = c("center", "scale"),
#                data = df_train)
#
# plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
#      main = "K vs Accuracy, 10 fold CV, CARET")
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
#library(DataExplorer)
#DataExplorer::create_report(df, y = 'Type')
# shuffle data to minimize random effects
shuffled_df <-  df[sample(nrow(df)),]
# create training 80% and test 20%,
train <- createDataPartition(shuffled_df$Type, p = 0.8, list = FALSE, times = 1)
df_train <- shuffled_df[train,]
df_test <- shuffled_df[-train,]
# k (number of neighbors) to test, try odd numbers, to avoid ties
kmax <- 9
# build the model
model <- train(as.factor(Type)~.,
df_train,
method = "knn", # choose knn model
trControl=trainControl(
method="repeatedcv", # k-fold cross validation
number=10, # number of folds (k in cross validation)
repeats=5), # number of times to repeat k-fold cross validation
preProcess = c("center", "scale"), # standardize the data
tuneLength = kmax) # max number of neighbors (k in nearest neighbor)
plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
main = "K vs Accuracy, 10 fold CV, CARET")
# # tell caret to use cross validation with 10 folds
# trainctl <- trainControl(method = 'cv', number = 10)
#
# # build the model, k values 1-25, careful to scale the data using preproc
# model <- train(as.factor(Type)~.,
#                method = 'knn',
#                tuneGrid = expand.grid(k = 1:25),
#                trControl = trainctl,
#                metric = 'Accuracy',
#                preProc = c("center", "scale"),
#                data = df_train)
#
# plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
#      main = "K vs Accuracy, 10 fold CV, CARET")
View(model)
model[["results"]][["k"]]
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
#library(DataExplorer)
#DataExplorer::create_report(df, y = 'Type')
# shuffle data to minimize random effects
shuffled_df <-  df[sample(nrow(df)),]
# create training 80% and test 20%,
train <- createDataPartition(shuffled_df$Type, p = 0.8, list = FALSE, times = 1)
df_train <- shuffled_df[train,]
df_test <- shuffled_df[-train,]
# k (number of neighbors) to test, try odd numbers, to avoid ties
kmax <- 15
# build the model
model <- train(as.factor(Type)~.,
df_train,
method = "knn", # choose knn model
trControl=trainControl(
method="repeatedcv", # k-fold cross validation
number=10, # number of folds (k in cross validation)
repeats=5), # number of times to repeat k-fold cross validation
preProcess = c("center", "scale"), # standardize the data
tuneGrid = expand.grid(k=3:kmax)) # max number of neighbors (k in nearest neighbor)
plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
main = "K vs Accuracy, 10 fold CV, CARET")
# # tell caret to use cross validation with 10 folds
# trainctl <- trainControl(method = 'cv', number = 10)
#
# # build the model, k values 1-25, careful to scale the data using preproc
# model <- train(as.factor(Type)~.,
#                method = 'knn',
#                tuneGrid = expand.grid(k = 1:25),
#                trControl = trainctl,
#                metric = 'Accuracy',
#                preProc = c("center", "scale"),
#                data = df_train)
#
# plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
#      main = "K vs Accuracy, 10 fold CV, CARET")
View(model)
model[["results"]][["Accuracy"]]
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
library(DataExplorer)
DataExplorer::create_report(df, y = 'Type')
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=FALSE)
data <-  data.matrix(df)
# Optional Explore the data quickly
library(DataExplorer)
DataExplorer::create_report(df, y = df[,10])
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
library(DataExplorer)
DataExplorer::create_report(df, y = df[,10])
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
library(DataExplorer)
DataExplorer::create_report(df, y = 'Type')
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
library(DataExplorer)
DataExplorer::create_report(df)
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
#library(DataExplorer)
#DataExplorer::create_report(df)
# shuffle data to minimize random effects
shuffled_df <-  df[sample(nrow(df)),]
# create training 80% and test 20%,
train <- createDataPartition(shuffled_df$Type, p = 0.8, list = FALSE, times = 1)
df_train <- shuffled_df[train,]
df_test <- shuffled_df[-train,]
# k (number of neighbors) to test, try odd numbers, to avoid ties
kmax <- 15
# build the model
model <- train(as.factor(Type)~.,
df_train,
method = "knn", # choose knn model
trControl=trainControl(
method="repeatedcv", # k-fold cross validation
number=10, # number of folds (k in cross validation)
repeats=5), # number of times to repeat k-fold cross validation
preProcess = c("center", "scale"), # standardize the data
tuneGrid = expand.grid(k=3:kmax)) # max number of neighbors (k in nearest neighbor)
plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
main = "K vs Accuracy, 10 fold CV, CARET")
dev.new()
ggplot(df, aes(Al, Ba, color = as.factor(Type))) + geom_point() +ggtitle("3 clusters, nstart=5")
# # tell caret to use cross validation with 10 folds
# trainctl <- trainControl(method = 'cv', number = 10)
#
# # build the model, k values 1-25, careful to scale the data using preproc
# model <- train(as.factor(Type)~.,
#                method = 'knn',
#                tuneGrid = expand.grid(k = 1:25),
#                trControl = trainctl,
#                metric = 'Accuracy',
#                preProc = c("center", "scale"),
#                data = df_train)
#
# plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
#      main = "K vs Accuracy, 10 fold CV, CARET")
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
#library(DataExplorer)
#DataExplorer::create_report(df)
# shuffle data to minimize random effects
shuffled_df <-  df[sample(nrow(df)),]
# create training 80% and test 20%,
train <- createDataPartition(shuffled_df$Type, p = 0.8, list = FALSE, times = 1)
df_train <- shuffled_df[train,]
df_test <- shuffled_df[-train,]
# k (number of neighbors) to test, try odd numbers, to avoid ties
kmax <- 15
# build the model
model <- train(as.factor(Type)~.,
df_train,
method = "knn", # choose knn model
trControl=trainControl(
method="repeatedcv", # k-fold cross validation
number=10, # number of folds (k in cross validation)
repeats=5), # number of times to repeat k-fold cross validation
preProcess = c("center", "scale"), # standardize the data
tuneGrid = expand.grid(k=3:kmax)) # max number of neighbors (k in nearest neighbor)
plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
main = "K vs Accuracy, 10 fold CV, CARET")
dev.new()
ggplot(df, aes(Al, Na, color = as.factor(Type))) + geom_point() +ggtitle("Al, Na vs TYpe")
# # tell caret to use cross validation with 10 folds
# trainctl <- trainControl(method = 'cv', number = 10)
#
# # build the model, k values 1-25, careful to scale the data using preproc
# model <- train(as.factor(Type)~.,
#                method = 'knn',
#                tuneGrid = expand.grid(k = 1:25),
#                trControl = trainctl,
#                metric = 'Accuracy',
#                preProc = c("center", "scale"),
#                data = df_train)
#
# plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
#      main = "K vs Accuracy, 10 fold CV, CARET")
# clear variables, load libraries, set seed
rm(list = ls())
#library(kernlab)
#library(kknn)
library(caret)
set.seed(13)
# read the data
df <-  read.csv('./glass.csv', header=TRUE)
data <-  data.matrix(df)
# Optional Explore the data quickly
#library(DataExplorer)
#DataExplorer::create_report(df)
# shuffle data to minimize random effects
shuffled_df <-  df[sample(nrow(df)),]
# create training 80% and test 20%,
train <- createDataPartition(shuffled_df$Type, p = 0.8, list = FALSE, times = 1)
df_train <- shuffled_df[train,]
df_test <- shuffled_df[-train,]
# k (number of neighbors) to test, try odd numbers, to avoid ties
kmax <- 15
# build the model
model <- train(as.factor(Type)~.,
df_train,
method = "knn", # choose knn model
trControl=trainControl(
method="repeatedcv", # k-fold cross validation
number=10, # number of folds (k in cross validation)
repeats=5), # number of times to repeat k-fold cross validation
preProcess = c("center", "scale"), # standardize the data
tuneGrid = expand.grid(k=3:kmax)) # max number of neighbors (k in nearest neighbor)
plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
main = "K vs Accuracy, 10 fold CV, CARET")
dev.new()
ggplot(df, aes(Al, Mg, color = as.factor(Type))) + geom_point() +ggtitle("Al, Na vs TYpe")
# # tell caret to use cross validation with 10 folds
# trainctl <- trainControl(method = 'cv', number = 10)
#
# # build the model, k values 1-25, careful to scale the data using preproc
# model <- train(as.factor(Type)~.,
#                method = 'knn',
#                tuneGrid = expand.grid(k = 1:25),
#                trControl = trainctl,
#                metric = 'Accuracy',
#                preProc = c("center", "scale"),
#                data = df_train)
#
# plot(model$results$k, model$results$Accuracy, type = "o", col = "blue", xlab = "K value", ylab = "Accuracy",
#      main = "K vs Accuracy, 10 fold CV, CARET")
